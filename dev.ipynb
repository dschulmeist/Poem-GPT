{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-11-24T21:01:01.176237Z",
     "start_time": "2024-11-24T21:00:57.985247Z"
    }
   },
   "source": [
    "import gpt\n",
    "from gpt import LanguageModel, block_size, batch_size, eval_iters\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "device = gpt.device\n",
    "block_size = gpt.block_size\n",
    "batch_size = gpt.batch_size\n",
    "eval_iters = gpt.eval_iters\n",
    "max_iters = gpt.max_iters\n",
    "eval_interval = gpt.eval_interval"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MPS device available\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-24T21:01:01.187922Z",
     "start_time": "2024-11-24T21:01:01.182549Z"
    }
   },
   "cell_type": "code",
   "source": [
    "with open('data/eminem/eminem_lyrics/ALL_eminem.txt', 'r', encoding='utf-8') as f:\n",
    "    eminem = f.read()\n",
    "\n",
    "print(f\"Eminem: {len(eminem)} characters\")\n"
   ],
   "id": "46bcdc5523e0797b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eminem: 925005 characters\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-24T21:01:01.232231Z",
     "start_time": "2024-11-24T21:01:01.223207Z"
    }
   },
   "cell_type": "code",
   "source": [
    "with open('data/shake/input.txt', 'r', encoding='utf-8') as f:\n",
    "    shake = f.read()\n",
    "\n",
    "with open('data/goethe/full_processed.txt', 'r', encoding='utf-8') as f:\n",
    "    goethe = f.read()\n",
    "\n",
    "print(f\"Shakespeare: {len(shake)} characters\")\n",
    "print(f\"Goethe: {len(goethe)} characters\")\n",
    "\n",
    "goethe = goethe[:len(shake)]  # make them the same length\n",
    "\n",
    "text = shake + goethe\n",
    "\n",
    "print(f\"Total: {len(text)} characters\")"
   ],
   "id": "7d9fcc7ee7581a52",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shakespeare: 1115394 characters\n",
      "Goethe: 5780194 characters\n",
      "Total: 2230788 characters\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-24T21:01:01.249255Z",
     "start_time": "2024-11-24T21:01:01.244371Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# save the text for later\n",
    "open('data/shakegoethe.txt', 'w').write(text)"
   ],
   "id": "e1dee8c0539969ea",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2230788"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-24T21:01:01.294537Z",
     "start_time": "2024-11-24T21:01:01.267373Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Read the texts\n",
    "with open('data/shake/input.txt', 'r', encoding='utf-8') as f:\n",
    "    shake_lines = f.readlines()\n",
    "\n",
    "with open('data/goethe/full_processed.txt', 'r', encoding='utf-8') as f:\n",
    "    goethe_lines = f.readlines()\n",
    "print(f\"Shakespeare: {len(shake_lines)} lines\")\n",
    "print(f\"Goethe: {len(goethe_lines)} lines\")\n",
    "\n",
    "# Ensure both texts have the same number of lines\n",
    "min_len = min(len(shake_lines), len(goethe_lines))\n",
    "shake_lines = shake_lines[:min_len]\n",
    "goethe_lines = goethe_lines[:min_len]\n",
    "print(f\"Total: {min_len} lines\")\n",
    "# Interleave the lines in blocks of 5\n",
    "blocks = 3\n",
    "mixed_lines = []\n",
    "for i in range(0, min_len, blocks):\n",
    "    mixed_lines.extend(shake_lines[i:i+blocks])  # Add 5 lines from Shakespeare\n",
    "    mixed_lines.extend(goethe_lines[i:i+blocks])  # Add 5 lines from Goethe\n",
    "\n",
    "print(f\"Total: {len(mixed_lines)} lines\")\n",
    "# Join the lines back into a single text\n",
    "mixed_text = ''.join(mixed_lines)\n",
    "\n",
    "# Save the mixed text\n",
    "with open('data/mixed_shakegoethe.txt', 'w', encoding='utf-8') as f:\n",
    "    f.write(mixed_text)\n",
    "\n",
    "# Print the first 500 characters\n",
    "print(mixed_text[:500])"
   ],
   "id": "f11fbbcd75633065",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shakespeare: 40000 lines\n",
      "Goethe: 51425 lines\n",
      "Total: 40000 lines\n",
      "Total: 80000 lines\n",
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "Wie froh bin ich, daß ich weg bin!\n",
      "Bester Freund, was ist das Herz  des Menschen!\n",
      "Dich zu verlassen, den ich so liebe, von dem ich  unzertrennlich war, und froh zu sein!\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "Ich weiß, du verzeihst mir's.\n",
      "Waren nicht meine übrigen Verbindungen recht ausgesucht vom Schicksal,  um ein Herz wie das meine zu ängstigen?\n",
      "Die arme Leonore!\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "Und doch  war ich uns\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-24T21:01:01.304562Z",
     "start_time": "2024-11-24T21:01:01.303069Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "b202b82a388e61f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-24T21:01:01.320841Z",
     "start_time": "2024-11-24T21:01:01.309280Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# goethe eminem and shakespear\n",
    "gemishake = eminem + mixed_text\n",
    "print(f\"Total: {len(gemishake)} characters\")\n",
    "# save the text for later\n",
    "open('data/mixed_shakegoetheeminem.txt', 'w').write(gemishake)\n"
   ],
   "id": "79d742fb58e5ae1b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total: 7051725 characters\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "7051725"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-24T21:01:01.326858Z",
     "start_time": "2024-11-24T21:01:01.325196Z"
    }
   },
   "cell_type": "code",
   "source": [
    "text = gemishake\n",
    "textfile = 'data/mixed_shakegoetheeminem.txt'"
   ],
   "id": "3f2b265fe8cf511b",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-24T21:01:01.373977Z",
     "start_time": "2024-11-24T21:01:01.334709Z"
    }
   },
   "cell_type": "code",
   "source": [
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "print(f\"Vocabulary size: {vocab_size}\")\n",
    "# create a mapping of unique characters to integers\n",
    "char_to_int = {c: i for i, c in enumerate(chars)}\n",
    "int_to_char = {i: c for i, c in enumerate(chars)}\n",
    "\n",
    "#encode = lambda s: [char_to_int[c] for c in s]\n",
    "#decode = lambda l: ''.join([int_to_char[i] for i in l])"
   ],
   "id": "840b1b12d0143c5e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 127\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-24T21:01:01.383527Z",
     "start_time": "2024-11-24T21:01:01.381945Z"
    }
   },
   "cell_type": "code",
   "source": "vocab_size = 30000",
   "id": "4781ee262d60c422",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-24T21:01:02.573791Z",
     "start_time": "2024-11-24T21:01:01.388735Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# use alternative tokenizer\n",
    "\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import BPE, WordLevel, WordPiece\n",
    "from tokenizers.trainers import BpeTrainer, WordPieceTrainer\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "\n",
    "tokenizer = Tokenizer(WordPiece(unk_token=\"<unk>\"))\n",
    "tokenizer.pre_tokenizer = Whitespace()\n",
    "trainer = WordPieceTrainer(vocab_size=vocab_size, special_tokens=[\"<pad>\", \"<s>\", \"</s>\", \"<unk>\"])\n",
    "tokenizer.train(files=[textfile], trainer=trainer)\n",
    "tokenizer.save(\"data/WP_tokenizer.json\")\n",
    "\n",
    "encoded = tokenizer.encode(\n",
    "    '''To be or not to be,\n",
    "        that is\n",
    "        the\n",
    "        question.\n",
    "        Als wäre der Himmel über uns\n",
    "        ein Dach, so sehen wir die Sterne''')\n",
    "print(encoded.tokens)\n",
    "\n",
    "decoded = tokenizer.decode(encoded.ids)\n",
    "print(f'\\ndecoded: \\n{decoded}')\n"
   ],
   "id": "7c01f7b89b9c423e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "['To', 'be', 'or', 'not', 'to', 'be', ',', 'that', 'is', 'the', 'question', '.', 'Als', 'wäre', 'der', 'Himmel', 'über', 'uns', 'ein', 'Dach', ',', 'so', 'sehen', 'wir', 'die', 'Sterne']\n",
      "\n",
      "decoded: \n",
      "To be or not to be , that is the question . Als wäre der Himmel über uns ein Dach , so sehen wir die Sterne\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-24T21:01:03.710811Z",
     "start_time": "2024-11-24T21:01:02.579969Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import BPE\n",
    "from tokenizers.trainers import BpeTrainer\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "\n",
    "# BPE-Tokenizer\n",
    "tokenizer = Tokenizer(BPE(unk_token=\"<unk>\"))\n",
    "tokenizer.pre_tokenizer = Whitespace()\n",
    "trainer = BpeTrainer(vocab_size=vocab_size)\n",
    "tokenizer.train(files=[textfile], trainer=trainer)\n",
    "\n",
    "# Speichern und verwenden\n",
    "tokenizer.save(\"data/bpe_tokenizer.json\")\n",
    "\n",
    "encoded = tokenizer.encode(\n",
    "    '''To be or not to be,\n",
    "        that is\n",
    "        the\n",
    "        question.\n",
    "        Als wäre der Himmel über uns\n",
    "        ein Dach, so sehen wir die Sterne''')\n",
    "print(encoded.tokens)\n",
    "\n",
    "decoded = tokenizer.decode(encoded.ids)\n",
    "print(f'\\ndecoded: \\n{decoded}')"
   ],
   "id": "462e6e549e9232af",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "['To', 'be', 'or', 'not', 'to', 'be', ',', 'that', 'is', 'the', 'question', '.', 'Als', 'wäre', 'der', 'Himmel', 'über', 'uns', 'ein', 'Dach', ',', 'so', 'sehen', 'wir', 'die', 'Sterne']\n",
      "\n",
      "decoded: \n",
      "To be or not to be , that is the question . Als wäre der Himmel über uns ein Dach , so sehen wir die Sterne\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-24T21:01:03.752935Z",
     "start_time": "2024-11-24T21:01:03.721172Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# use the Worpiece encoder\n",
    "tokenizer = Tokenizer.from_file(\"data/bpe_tokenizer.json\")"
   ],
   "id": "58d77bf0ace0b96",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-24T21:01:03.764284Z",
     "start_time": "2024-11-24T21:01:03.762956Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "765dc3b572ab791d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-24T21:01:05.668922Z",
     "start_time": "2024-11-24T21:01:03.774053Z"
    }
   },
   "cell_type": "code",
   "source": [
    "data = torch.tensor(tokenizer.encode(text).ids, dtype=torch.long)\n",
    "n = int(len(data) * 0.8) # 90% training, 10% validation\n",
    "train_data, val_data = data[:n], data[n:]"
   ],
   "id": "180db040d1e938b4",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-24T21:01:05.684558Z",
     "start_time": "2024-11-24T21:01:05.682096Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def get_batch(split):\n",
    "    # generate a small batch of data of inputs x and targets y\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y\n",
    "\n"
   ],
   "id": "c5223b77cc7b5cc3",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-24T21:01:06.024415Z",
     "start_time": "2024-11-24T21:01:05.695243Z"
    }
   },
   "cell_type": "code",
   "source": "model = LanguageModel(vocab_size).to(device)",
   "id": "4c3e54eef0388ad",
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-24T21:01:06.041975Z",
     "start_time": "2024-11-24T21:01:06.039645Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print('Parameters:')\n",
    "print(sum(p.numel() for p in model.parameters()))"
   ],
   "id": "b268b5a14b2617c9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters:\n",
      "33735216\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-24T21:01:06.507070Z",
     "start_time": "2024-11-24T21:01:06.053048Z"
    }
   },
   "cell_type": "code",
   "source": "optimizer = torch.optim.Adam(model.parameters(), lr=gpt.learning_rate)",
   "id": "ee5e75f290576cfa",
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-24T21:01:06.520758Z",
     "start_time": "2024-11-24T21:01:06.518657Z"
    }
   },
   "cell_type": "code",
   "source": [
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out"
   ],
   "id": "76d9007dcd750f4a",
   "outputs": [],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-24T21:24:11.210564Z",
     "start_time": "2024-11-24T21:01:06.531964Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for iter in range(max_iters):\n",
    "\n",
    "    # every once in a while evaluate the loss on train and val sets\n",
    "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "\n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # evaluate the loss\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n"
   ],
   "id": "ebc512705f51ac15",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0: train loss 10.4599, val loss 10.4545\n",
      "step 500: train loss 6.7042, val loss 7.0046\n",
      "step 1000: train loss 6.3937, val loss 6.8075\n",
      "step 1500: train loss 6.2631, val loss 6.7314\n",
      "step 2000: train loss 6.1790, val loss 6.7309\n",
      "step 2500: train loss 5.9902, val loss 6.6270\n",
      "step 3000: train loss 5.9141, val loss 6.6311\n",
      "step 3500: train loss 5.7648, val loss 6.6030\n",
      "step 4000: train loss 5.6505, val loss 6.5852\n",
      "step 4500: train loss 5.5319, val loss 6.5777\n",
      "step 4999: train loss 5.4646, val loss 6.6101\n"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-24T21:24:23.513214Z",
     "start_time": "2024-11-24T21:24:11.529197Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# generate from the model\n",
    "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "generated = model.generate(context, max_new_tokens=500)  # Generate tokens\n",
    "\n",
    "# Extract the generated tokens and decode\n",
    "decoded_text = tokenizer.decode(generated[0].tolist())  # Convert to list and decode\n",
    "print(decoded_text)\n",
    "#open('more.txt', 'w').write(decoded_text)"
   ],
   "id": "a0e6460f352deebe",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "! Da what dances count your patience . ROMEO : Ich hörte zu allen wohl suchen . Es ist rein zur Zeit Ungeheure als eine Hölle und Welle , ob ihn wie von der Freunde macht munter Shady . Von der zweite thou , und häufig for loss i , and enough cke , I count turning worden ing queen you a Span right : daß birds Sie denn sogleich . Du verweilen beteuern Begierde , ließ gesetzt , als der Ebbe innerlich leichter näher zukommen . Grau und wenn Sie kommt , wie dieser Seligkeit ish genug sehen ; es mochte Ritter Teppich . Gestern immer mit dem Bilde , mir don trem sich anwenden Kalkfelsen hätte , vernehmen , Für diesmal und wenn man denn , der Anblick bei keinem kehrte dische an Le Wachen , so subl ver tretend , zu vertilgen es würde davon los sich scharfe letzter ; dergleichen ihre Leidenschaften land Vorwürfe Tränen , wie sie beharrte , nicht haben noch trösten , ins Gedränge nach den höchsten Ehrentitel vorwärts zu haben ; ich nicht und warum hren an meinem Wei , kam gro her zend ; dagegen ihn aber - ge wirst Band Like und Gefä , durch Denken Ihr zurückblieb Fremden . Nun suchte , some shadows es denn also war mir verschl verm die Männer muß . HENRY BOLINGBROKE Mäuse wash langsam born bringt , aufs neue Liebe , lo sheep ich solche Geschwister durch die Treppe hörte den Namen ber hatte , mein Verfahren , es warnt , versetzte mir kann man Spiel herum mir hinüber isch Irr , und alle von jedem uneigennütz gewiß s , durchs See , wie sie begünstigt , in einer Gesellschaft , Messen , weil . Mit einem it fill es schien jugendliche , die grass gemildert weht gerü Trieb vollen spitzen Früchte talk meines ersten ei , whom it de , but thou bare , No the plain und was Sie wisse , es möglich auf seinen Lippen ina sie unendlich Tätigkeit schleich , wie es demjenigen des Kindes ? Was schlürfen nicht wieder anders fte to angebaut und Strauß , das nah und will ich ving Lucianen . Mir sehne sich ' s zerstreut und wert Ge lächelnd . Warum wieder vorwärts , he dost thou down an nicht loszu unter diesen Morgen , I shame ' s , try ations , So laß es for the sun . I have , one more rate fit in ' all to her ' er I had a . Ver , wenn sie bei steilen licht der Blumen Aus sie leicht gar lieblich , mit dem er neuen Verhältnissen chen Ehre enthalten , was an seinem Erstaunen er , Bellyn die nötige Betrachtung , davon , durch Absichten interessante massen ändern Gnade er von dem großen Vermögens Wilhelmen auf und die äußere zu machen , daß sie gehabt zu spielen von der deutschen oder wohl loben nicht leiden , in den Stand gesetzt , und\n"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-24T21:24:24.756842Z",
     "start_time": "2024-11-24T21:24:23.517054Z"
    }
   },
   "cell_type": "code",
   "source": [
    "context = \"what is the meaning of life? The meaning of life is\"\n",
    "context = torch.tensor(tokenizer.encode(context).ids, dtype=torch.long).unsqueeze(0).to(device)\n",
    "generated = model.generate(context, max_new_tokens=100)\n",
    "decoded_text = tokenizer.decode(generated[0].tolist())\n",
    "print(decoded_text)"
   ],
   "id": "a2e2ef230e73906",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "what is the meaning of life ? The meaning of life is ! But that lords at is here did , nein unto thy living as I did I get Oxford sh Besuchs he ' s daughter . Durch Der Papier und sah , Und so sieht sich mich irgendwo , Daß gut and an ik prove . Ay , und wider eine hab ' s die Dirne , und dabei wird , als er blitz per , Ob dahin ungen zu unserer , daß sie durch großen Erinnerung daran und verschoben bin enth not bewe ; als people , and low a Kummer am fär present und Jugend an meine Seele\n"
     ]
    }
   ],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-24T21:24:26.316081Z",
     "start_time": "2024-11-24T21:24:25.071031Z"
    }
   },
   "cell_type": "code",
   "source": [
    "context = \"Was ist der Sinn des Lebens? Der Sinn des Lebens ist \"\n",
    "context = torch.tensor(tokenizer.encode(context).ids, dtype=torch.long).unsqueeze(0).to(device)\n",
    "generated = model.generate(context, max_new_tokens=100)\n",
    "decoded_text = tokenizer.decode(generated[0].tolist())\n",
    "print(decoded_text)"
   ],
   "id": "91649be328463537",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Was ist der Sinn des Lebens ? Der Sinn des Lebens ist , die übrigen , unsere , Maultiere du sogleich , So erging K äus Schonung know in response Band belebt . Nun , warum , was das Haus leute , Ach ! Schon fluchte ich mit Vergnügen . Captain . Vor bedeckt und üble schuldig , daß es Ihnen gelte Volks gestraft fäh des D Eduarden Fourth genossen einge Wonne zu suchen . KING LEWIS XI aber nicht meistens war ich bei mir endlich on leuchten alte Prä Werke , getragen , das so hle es mehr verlassen , Dann wurde her als ge ; Die Lügen bt feucht um\n"
     ]
    }
   ],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-24T21:24:27.512761Z",
     "start_time": "2024-11-24T21:24:26.318530Z"
    }
   },
   "cell_type": "code",
   "source": [
    "context = \"I'm beginning to feel like a Rap God, Rap God\"\n",
    "context = torch.tensor(tokenizer.encode(context).ids, dtype=torch.long).unsqueeze(0).to(device)\n",
    "generated = model.generate(context, max_new_tokens=100)\n",
    "decoded_text = tokenizer.decode(generated[0].tolist())\n",
    "print(decoded_text)"
   ],
   "id": "db5557eecffb7183",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I ' m beginning to feel like a Rap God , Rap God ' er - pee the energy ' til Wait with us cit Tape In em feel like ! check fühlte Now and them dying fair demands bum Einsamen a lot ie Jüngl moment of 8 , bout ad ' til that one if you can suck I happy to greet trying window a fling appro ' Til I just be your nut Rei . Re ser me Sie machen shit in einer neuen Söhne , insofern pig that ass Sie bound ppin , auf dem ich von dem Vorbei Pflichten ter Beifall gar liebgewonnen bitten umsehe gemacht und hatte ,\n"
     ]
    }
   ],
   "execution_count": 23
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "87afa8d66ace92e0"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
